# -*- coding: utf-8 -*-
"""MLHW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QrAyLPBVIvxqnywANyz7PCbk4ASJ0Ugb
"""

# Code to read csv file into Colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import copy


# Authenticate and create the PyDrive client.  #用來驗證
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#這邊的link是檔案共用連結
link = 'https://drive.google.com/open?id=1G1yVlLLozgkdAsBfCl6FZTAj1YN2KnYh'  
# Verify that you have everything after '='
fluff, id = link.split('=') 


#這邊要放檔案名稱
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('train.csv') 
d = pd.read_csv('train.csv', header=0,encoding = 'unicode_escape')

d.shape

# drop by column(index)
d.drop(d.columns[[0,1,2]], axis = 1, inplace = True)
d = d.replace('NR', 0)
pre_data = np.array(d, dtype = 'float')

def extract(pre_data, train):
    
    #第一部分, 將資料做拼接
    data = np.zeros((18,1)) # 先建立一個data set
    temp = pre_data[0,:]
    for i in range(1,pre_data.shape[0]):
        if i%18 == 0:
            data = np.hstack((data, temp))
            temp = pre_data[i,:]
        else:
            temp = np.vstack((temp, pre_data[i,:]))
    data = np.hstack((data, temp))
    data = np.delete(data,0,axis = 1) 
    
    #第二部分, 將每9個小時的資料接起來作為訓練集(測試集)
    x_train = np.zeros((1,18*9))
    
    if train == True:
        y_train = []
        for i in range(data.shape[1] - 10):
            #處理y_train
            y_train.append(data[9,i + 9])
            
            #處理x_train
            temp = np.transpose(data[:,i:i+1])
            for j in range(i+1,i+9):
                temp = np.hstack((temp, np.transpose(data[:,j:j+1])))
            x_train = np.vstack((x_train,temp))
        x_train = np.delete(x_train,0,axis = 0) 
        y_train = np.array(y_train).reshape((5750,1)) #把y轉成array
        return x_train, y_train
    else:
        for i in range(0,data.shape[1],9):
            
            #處理x_train
            temp = np.transpose(data[:,i:i+1])
            for j in range(i+1,i+9):
                temp = np.hstack((temp, np.transpose(data[:,j:j+1])))
            x_train = np.vstack((x_train,temp))
        x_train = np.delete(x_train,0,axis = 0) 
        return x_train

X, Y = extract(pre_data, True)

"""做data的Preprocessing, 包含normalization, 取出outlier, 以及在最前面加上1"""

def normalize(data_real, mean = None, std = None,train = True):
    
  data = copy.copy(data_real)
  if train == True:
    mean = data.mean(axis = 0) # by column
    std = data.std(axis = 0)
    for i in range(data.shape[1]):
      data[:,i] = (data[:,i] - mean[i]) / std[i]
    return data, mean, std
  
  else:
    for i in range(data.shape[1]):
      data[:,i] = (data[:,i] - mean[i]) / std[i]
  return data

#做normalize
X, Y = extract(pre_data, True)
X_train, train_mean, train_std = normalize(X)

#取出outlier
threshold = 10
outlier_index = np.unique(np.where(abs(X_train) > threshold)[0])
X_train = np.delete(X_train, outlier_index, axis = 0)
Y = np.delete(Y, outlier_index, axis = 0)
print("Original:", X.shape[0], "After:",X_train.shape[0])

#前面加上一, 方便矩陣操作  why?????
X_train = np.concatenate((np.ones((X_train.shape[0], 1 )), X_train) , axis = 1).astype(float)

"""這邊用Linear gradient descent的方式跑跑看"""

#訓練模型的部分 Adam
#最好的成績為 5.813
weight = np.zeros(shape = (X_train.shape[1],1))
lr = 0.001 
b1 = 0.9
b2 = 0.99
m = 0
v = 0
t = 1
bias = 0
loss_list = []
iteration = 100000

#紀錄weight的
weight_list = []
stop_point = [5000, 10000, 15000, 20000]

for i in range(iteration):
  y_hat = X_train.dot(weight)
  loss = Y - X_train.dot(weight)
  loss_list.append(np.power(np.sum(np.power(loss,2)),0.5))
  gradient = -2 * ( X_train.T.dot(loss) )
  m = b1*m + (1 - b1)*gradient
  v = b2*v + (1 - b2)*(gradient**2)
  m_hat = m / (1 - np.power(b1, t))
  v_hat = v / (1 - np.power(b2, t))
  weight = weight - (lr * m_hat) / (np.sqrt(v_hat))
  t += 1
  print("The epoch is",i ,"and Loss :", loss_list[-1])
  
  #這邊做紀錄
  # epoch:20317, grade = 6.03
  # epoch:21517, grade = 5.83
  # epoch:22000, grade = 5.81
  # epoch:23000, grade = 5.81329
  # epoch:30000, grade = 5.81297
  
  
  #控制loss, 不要讓它overfitting
  #在沒有進行outlier的刪減時, Loss最低為439
  #進行outlier的刪減之後（Threshold為3.5）, Loss最低為328
  
  
  
  #if loss_list[-1] < 329:  #epoch在21400左右
  #  break
  if i in stop_point:
    weight_list.append(weight)
  
  if i > 25000:
    break

weight = weight_list[1]

plt.title("Linear regression")
plt.plot(loss_list)

#訓練模型的部分  adagrad   
#訓練成效比SGD還差...
weight = np.zeros(shape = (X_train.shape[1],1))
lr = 0.01
prev_gra = 0
loss_list = []

iteration = 10000

for i in range(iteration):
  y_hat = X_train.dot(weight)
  loss = Y - X_train.dot(weight)
  loss_list.append(np.power(np.sum(np.power(loss,2)),0.5))
  gradient = -2 * ( X_train.T.dot(loss) )
  prev_gra += gradient**2
  ada = np.sqrt(prev_gra)
  weight = weight - lr * gradient/ada

plt.title("Linear regression")
plt.plot(loss_list)

weight = temp

#SGD
#最底大概到：439
weight = np.zeros(shape = (X_train.shape[1],1))
bias = 0
lr = 0.0000001
loss_list = []

weight_list = []
stop_point = [3000, 6000, 9000, 12000, 15000, 18000] 

iteration = 130000

for i in range(iteration):
  y_hat = X_train.dot(weight)
  loss = Y - X_train.dot(weight) - bias
  loss_list.append(np.power(np.sum(np.power(loss,2)),0.5))
  
  w_gradient = -2 * ( X_train.T.dot(loss) )
  #b_gradient = -2 * np.sum(loss)
  
  
  weight = weight - lr * w_gradient
  #bias = bias - lr * b_gradient
 
  if i%2000 == 0:
    print("The epoch is",i ,"and Loss :", loss_list[-1])
  
  if i in stop_point:
    weight_list.append(weight)

weight = weight_list[5]

plt.title("Linear regression")
plt.plot(loss_list)

# Closed form solution
from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(X_train, Y)
d_test_predict = reg.predict(d_test_preprocess)

#這邊的link是檔案共用連結
link = 'https://drive.google.com/open?id=1-_yMbdeXF3kANGr1HAgXxHl4VLVaYql6'  
# Verify that you have everything after '='
fluff, id = link.split('=') 


#這邊要放檔案名稱
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('test.csv') 
d_test = pd.read_csv('test.csv', header = None,encoding = 'unicode_escape')

d_test.drop(d_test.columns[[0,1]], axis = 1, inplace = True)
d_test = d_test.replace('NR', 0)
d_test = np.array(d_test, dtype = 'float')

d_test = extract(d_test, False)
d_test_preprocess = normalize(d_test, mean = train_mean, std = train_std, train = False)
d_test_preprocess = np.concatenate((np.ones((d_test_preprocess.shape[0], 1 )), d_test_preprocess) , axis = 1).astype(float)

d_test_predict = d_test_preprocess.dot(weight)
#d_test_predict = d_test_preprocess.dot(weight) + bias

"""試試看先跑PCA之後, 先把維度降低  --> 成效不彰"""

#import sklearn
#from sklearn.model_selection import train_test_split
#from sklearn.decomposition import PCA
#先進行標準化, 再進行PCA, 不會讓某些值影響的太大（剛剛一開始直接用PCA without normalization時, 數值會變成超級大）
#X_preprocess = sklearn.preprocessing.scale(X, axis = 1, with_mean = True)
#X_PCA = PCA(n_components = 70).fit_transform(X_preprocess)
#X_train_preprocess, X_test_preprocess, Y_train, Y_test = train_test_split(X_PCA, Y, test_size = 0.4, random_state = 299)

#X_train.shape

#validation
import sklearn
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.4, random_state = 5)

X_train_preprocess = sklearn.preprocessing.scale(X_train, axis = 1, with_mean=True)
X_test_preprocess = sklearn.preprocessing.scale(X_test, axis = 1, with_mean=True)

X_train.shape

"""以下是用Deep的方式"""

from keras import regularizers
from keras import backend as K
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, BatchNormalization
from keras.layers.advanced_activations import LeakyReLU

epochs = 100
batch_size = 128

model = Sequential()
model.add(Dense(64, input_shape = (X_train_preprocess.shape[1], )))
#model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001))
model.add(LeakyReLU(alpha =0.15))
#model.add(Activation('relu'))
model.add(Dropout(rate = 0.5))        
  
  
model.add(Dense(64))
#model.add(Activation('relu'))
model.add(LeakyReLU(alpha =0.15))
model.add(Dropout(rate = 0.5))

model.add(Dense(64))
#model.add(Activation('relu'))
model.add(LeakyReLU(alpha =0.15))
model.add(Dropout(rate = 0.5))

model.add(Dense(64))
#model.add(Activation('relu'))
model.add(LeakyReLU(alpha =0.15))
model.add(Dropout(rate = 0.5))

model.add(Dense(64))
model.add(LeakyReLU(alpha =0.15))
model.add(Dropout(rate = 0.5))



#自定義RMSE作為Loss function
def RMSE(y_true, y_pred):
        return K.sqrt(K.mean(K.square(y_pred - y_true))) 
          
model.add(Dense(1))
model.compile(optimizer = 'Nadam', loss = RMSE, metrics=['mae'])

train_histroy = model.fit(X_train_preprocess, Y_train, validation_data=(X_test_preprocess, Y_test), batch_size = batch_size, epochs = epochs)

#這邊的link是檔案共用連結
link = 'https://drive.google.com/open?id=1-_yMbdeXF3kANGr1HAgXxHl4VLVaYql6'  
# Verify that you have everything after '='
fluff, id = link.split('=') 


#這邊要放檔案名稱
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('test.csv') 
d_test = pd.read_csv('test.csv', header = None,encoding = 'unicode_escape')

d_test.drop(d_test.columns[[0,1]], axis = 1, inplace = True)
d_test = d_test.replace('NR', 0)
d_test = np.array(d_test, dtype = 'float')

#PCA
#d_test = sklearn.preprocessing.scale(d_test, axis = 1, with_mean=True)
#d_test_preprocess = PCA(n_components = 70).fit_transform(d_test)

d_test = extract(d_test, False)
d_test_preprocess = sklearn.preprocessing.scale(d_test, axis = 1, with_mean=True)

d_test_predict = model.predict(d_test_preprocess)

"""輸出"""

#這邊的link是檔案共用連結
link = 'https://drive.google.com/open?id=1V1HL51bvpRnzMDf3js4z9n6Lp8j6parJ'  
# Verify that you have everything after '='
fluff, id = link.split('=') 


#這邊要放檔案名稱
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('sampleSubmission.csv') 
sample_submission = pd.read_csv('sampleSubmission.csv', header = 0,encoding = 'unicode_escape')

for i in range(d_test_predict.shape[0]):
  if d_test_predict[i][0] > 0:  #把所有輸出的負數都弄成0, 因為pm2.5本身不可能有負數數值
    sample_submission.iloc[i,1] = d_test_predict[i][0]

from google.colab import files
sample_submission.to_csv('submit.csv', index = False)

files.download('submit.csv')